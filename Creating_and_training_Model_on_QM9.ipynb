{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf1dba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 2.8.0\n",
      "nfp 0+unknown\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import nfp\n",
    "\n",
    "print(f\"tensorflow {tf.__version__}\")\n",
    "print(f\"nfp {nfp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b840d520",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9441d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mol_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>mu</th>\n",
       "      <th>alpha</th>\n",
       "      <th>homo</th>\n",
       "      <th>lumo</th>\n",
       "      <th>gap</th>\n",
       "      <th>...</th>\n",
       "      <th>zpve</th>\n",
       "      <th>u0</th>\n",
       "      <th>u298</th>\n",
       "      <th>h298</th>\n",
       "      <th>g298</th>\n",
       "      <th>cv</th>\n",
       "      <th>u0_atom</th>\n",
       "      <th>u298_atom</th>\n",
       "      <th>h298_atom</th>\n",
       "      <th>g298_atom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gdb_1</td>\n",
       "      <td>C</td>\n",
       "      <td>157.71180</td>\n",
       "      <td>157.709970</td>\n",
       "      <td>157.706990</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13.21</td>\n",
       "      <td>-0.3877</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044749</td>\n",
       "      <td>-40.478930</td>\n",
       "      <td>-40.476062</td>\n",
       "      <td>-40.475117</td>\n",
       "      <td>-40.498597</td>\n",
       "      <td>6.469</td>\n",
       "      <td>-395.999595</td>\n",
       "      <td>-398.643290</td>\n",
       "      <td>-401.014647</td>\n",
       "      <td>-372.471772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gdb_2</td>\n",
       "      <td>N</td>\n",
       "      <td>293.60975</td>\n",
       "      <td>293.541110</td>\n",
       "      <td>191.393970</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>9.46</td>\n",
       "      <td>-0.2570</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.3399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034358</td>\n",
       "      <td>-56.525887</td>\n",
       "      <td>-56.523026</td>\n",
       "      <td>-56.522082</td>\n",
       "      <td>-56.544961</td>\n",
       "      <td>6.316</td>\n",
       "      <td>-276.861363</td>\n",
       "      <td>-278.620271</td>\n",
       "      <td>-280.399259</td>\n",
       "      <td>-259.338802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gdb_3</td>\n",
       "      <td>O</td>\n",
       "      <td>799.58812</td>\n",
       "      <td>437.903860</td>\n",
       "      <td>282.945450</td>\n",
       "      <td>1.8511</td>\n",
       "      <td>6.31</td>\n",
       "      <td>-0.2928</td>\n",
       "      <td>0.0687</td>\n",
       "      <td>0.3615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021375</td>\n",
       "      <td>-76.404702</td>\n",
       "      <td>-76.401867</td>\n",
       "      <td>-76.400922</td>\n",
       "      <td>-76.422349</td>\n",
       "      <td>6.002</td>\n",
       "      <td>-213.087624</td>\n",
       "      <td>-213.974294</td>\n",
       "      <td>-215.159658</td>\n",
       "      <td>-201.407171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gdb_4</td>\n",
       "      <td>C#C</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>35.610036</td>\n",
       "      <td>35.610036</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.28</td>\n",
       "      <td>-0.2845</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.3351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026841</td>\n",
       "      <td>-77.308427</td>\n",
       "      <td>-77.305527</td>\n",
       "      <td>-77.304583</td>\n",
       "      <td>-77.327429</td>\n",
       "      <td>8.574</td>\n",
       "      <td>-385.501997</td>\n",
       "      <td>-387.237686</td>\n",
       "      <td>-389.016047</td>\n",
       "      <td>-365.800724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gdb_5</td>\n",
       "      <td>C#N</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>44.593883</td>\n",
       "      <td>44.593883</td>\n",
       "      <td>2.8937</td>\n",
       "      <td>12.99</td>\n",
       "      <td>-0.3604</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.3796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016601</td>\n",
       "      <td>-93.411888</td>\n",
       "      <td>-93.409370</td>\n",
       "      <td>-93.408425</td>\n",
       "      <td>-93.431246</td>\n",
       "      <td>6.278</td>\n",
       "      <td>-301.820534</td>\n",
       "      <td>-302.906752</td>\n",
       "      <td>-304.091489</td>\n",
       "      <td>-288.720028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gdb_6</td>\n",
       "      <td>C=O</td>\n",
       "      <td>285.48839</td>\n",
       "      <td>38.982300</td>\n",
       "      <td>34.298920</td>\n",
       "      <td>2.1089</td>\n",
       "      <td>14.18</td>\n",
       "      <td>-0.2670</td>\n",
       "      <td>-0.0406</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026603</td>\n",
       "      <td>-114.483613</td>\n",
       "      <td>-114.480746</td>\n",
       "      <td>-114.479802</td>\n",
       "      <td>-114.505268</td>\n",
       "      <td>6.413</td>\n",
       "      <td>-358.756935</td>\n",
       "      <td>-360.512706</td>\n",
       "      <td>-362.291066</td>\n",
       "      <td>-340.464421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gdb_7</td>\n",
       "      <td>CC</td>\n",
       "      <td>80.46225</td>\n",
       "      <td>19.906490</td>\n",
       "      <td>19.906330</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>23.95</td>\n",
       "      <td>-0.3385</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074542</td>\n",
       "      <td>-79.764152</td>\n",
       "      <td>-79.760666</td>\n",
       "      <td>-79.759722</td>\n",
       "      <td>-79.787269</td>\n",
       "      <td>10.098</td>\n",
       "      <td>-670.788296</td>\n",
       "      <td>-675.710476</td>\n",
       "      <td>-679.860821</td>\n",
       "      <td>-626.927299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gdb_8</td>\n",
       "      <td>CO</td>\n",
       "      <td>127.83497</td>\n",
       "      <td>24.858720</td>\n",
       "      <td>23.978720</td>\n",
       "      <td>1.5258</td>\n",
       "      <td>16.97</td>\n",
       "      <td>-0.2653</td>\n",
       "      <td>0.0784</td>\n",
       "      <td>0.3437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051208</td>\n",
       "      <td>-115.679136</td>\n",
       "      <td>-115.675816</td>\n",
       "      <td>-115.674872</td>\n",
       "      <td>-115.701876</td>\n",
       "      <td>8.751</td>\n",
       "      <td>-481.106758</td>\n",
       "      <td>-484.355372</td>\n",
       "      <td>-487.319724</td>\n",
       "      <td>-450.124128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gdb_9</td>\n",
       "      <td>CC#C</td>\n",
       "      <td>160.28041</td>\n",
       "      <td>8.593230</td>\n",
       "      <td>8.593210</td>\n",
       "      <td>0.7156</td>\n",
       "      <td>28.78</td>\n",
       "      <td>-0.2609</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.3222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055410</td>\n",
       "      <td>-116.609549</td>\n",
       "      <td>-116.605550</td>\n",
       "      <td>-116.604606</td>\n",
       "      <td>-116.633775</td>\n",
       "      <td>12.482</td>\n",
       "      <td>-670.268091</td>\n",
       "      <td>-673.980434</td>\n",
       "      <td>-677.537155</td>\n",
       "      <td>-631.346845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gdb_10</td>\n",
       "      <td>CC#N</td>\n",
       "      <td>159.03567</td>\n",
       "      <td>9.223270</td>\n",
       "      <td>9.223240</td>\n",
       "      <td>3.8266</td>\n",
       "      <td>24.45</td>\n",
       "      <td>-0.3264</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.3640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045286</td>\n",
       "      <td>-132.718150</td>\n",
       "      <td>-132.714563</td>\n",
       "      <td>-132.713619</td>\n",
       "      <td>-132.742149</td>\n",
       "      <td>10.287</td>\n",
       "      <td>-589.812024</td>\n",
       "      <td>-592.893721</td>\n",
       "      <td>-595.857446</td>\n",
       "      <td>-557.125708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mol_id smiles          A           B           C      mu  alpha    homo  \\\n",
       "0   gdb_1      C  157.71180  157.709970  157.706990  0.0000  13.21 -0.3877   \n",
       "1   gdb_2      N  293.60975  293.541110  191.393970  1.6256   9.46 -0.2570   \n",
       "2   gdb_3      O  799.58812  437.903860  282.945450  1.8511   6.31 -0.2928   \n",
       "3   gdb_4    C#C    0.00000   35.610036   35.610036  0.0000  16.28 -0.2845   \n",
       "4   gdb_5    C#N    0.00000   44.593883   44.593883  2.8937  12.99 -0.3604   \n",
       "5   gdb_6    C=O  285.48839   38.982300   34.298920  2.1089  14.18 -0.2670   \n",
       "6   gdb_7     CC   80.46225   19.906490   19.906330  0.0000  23.95 -0.3385   \n",
       "7   gdb_8     CO  127.83497   24.858720   23.978720  1.5258  16.97 -0.2653   \n",
       "8   gdb_9   CC#C  160.28041    8.593230    8.593210  0.7156  28.78 -0.2609   \n",
       "9  gdb_10   CC#N  159.03567    9.223270    9.223240  3.8266  24.45 -0.3264   \n",
       "\n",
       "     lumo     gap  ...      zpve          u0        u298        h298  \\\n",
       "0  0.1171  0.5048  ...  0.044749  -40.478930  -40.476062  -40.475117   \n",
       "1  0.0829  0.3399  ...  0.034358  -56.525887  -56.523026  -56.522082   \n",
       "2  0.0687  0.3615  ...  0.021375  -76.404702  -76.401867  -76.400922   \n",
       "3  0.0506  0.3351  ...  0.026841  -77.308427  -77.305527  -77.304583   \n",
       "4  0.0191  0.3796  ...  0.016601  -93.411888  -93.409370  -93.408425   \n",
       "5 -0.0406  0.2263  ...  0.026603 -114.483613 -114.480746 -114.479802   \n",
       "6  0.1041  0.4426  ...  0.074542  -79.764152  -79.760666  -79.759722   \n",
       "7  0.0784  0.3437  ...  0.051208 -115.679136 -115.675816 -115.674872   \n",
       "8  0.0613  0.3222  ...  0.055410 -116.609549 -116.605550 -116.604606   \n",
       "9  0.0376  0.3640  ...  0.045286 -132.718150 -132.714563 -132.713619   \n",
       "\n",
       "         g298      cv     u0_atom   u298_atom   h298_atom   g298_atom  \n",
       "0  -40.498597   6.469 -395.999595 -398.643290 -401.014647 -372.471772  \n",
       "1  -56.544961   6.316 -276.861363 -278.620271 -280.399259 -259.338802  \n",
       "2  -76.422349   6.002 -213.087624 -213.974294 -215.159658 -201.407171  \n",
       "3  -77.327429   8.574 -385.501997 -387.237686 -389.016047 -365.800724  \n",
       "4  -93.431246   6.278 -301.820534 -302.906752 -304.091489 -288.720028  \n",
       "5 -114.505268   6.413 -358.756935 -360.512706 -362.291066 -340.464421  \n",
       "6  -79.787269  10.098 -670.788296 -675.710476 -679.860821 -626.927299  \n",
       "7 -115.701876   8.751 -481.106758 -484.355372 -487.319724 -450.124128  \n",
       "8 -116.633775  12.482 -670.268091 -673.980434 -677.537155 -631.346845  \n",
       "9 -132.742149  10.287 -589.812024 -592.893721 -595.857446 -557.125708  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qm9 = pd.read_csv(\"qm9.csv\")\n",
    "qm9.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e8cba",
   "metadata": {},
   "source": [
    "## Remove duplicate SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "531d7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "qm9 = qm9.drop_duplicates(subset='smiles', keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97769be",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "87c62ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93802, 20000, 20000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Following 15% validation, 15% testing, 70% training size\n",
    "\n",
    "valid, test, train = np.split(qm9.smiles.sample(frac=1., random_state=123), [20000, 40000])\n",
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ec8cf",
   "metadata": {},
   "source": [
    "## Define how to featurize the input molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0d5d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfp.preprocessing.mol_preprocessor import SmilesPreprocessor\n",
    "from nfp.preprocessing.features import get_ring_size\n",
    "\n",
    "\n",
    "def atom_featurizer(atom):\n",
    "    \"\"\" Return an string representing the atom type\n",
    "    \"\"\"\n",
    "\n",
    "    return str((\n",
    "        atom.GetSymbol(),\n",
    "        atom.GetIsAromatic(),\n",
    "        get_ring_size(atom, max_size=6),\n",
    "        atom.GetDegree(),\n",
    "        atom.GetTotalNumHs(includeNeighbors=True)\n",
    "    ))\n",
    "\n",
    "\n",
    "def bond_featurizer(bond, flipped=False):\n",
    "    \"\"\" Get a similar classification of the bond type.\n",
    "    Flipped indicates which 'direction' the bond edge is pointing. \"\"\"\n",
    "    \n",
    "    if not flipped:\n",
    "        atoms = \"{}-{}\".format(\n",
    "            *tuple((bond.GetBeginAtom().GetSymbol(),\n",
    "                    bond.GetEndAtom().GetSymbol())))\n",
    "    else:\n",
    "        atoms = \"{}-{}\".format(\n",
    "            *tuple((bond.GetEndAtom().GetSymbol(),\n",
    "                    bond.GetBeginAtom().GetSymbol())))\n",
    "    \n",
    "    btype = str(bond.GetBondType())\n",
    "    ring = 'R{}'.format(get_ring_size(bond, max_size=6)) if bond.IsInRing() else ''\n",
    "    \n",
    "    return \" \".join([atoms, btype, ring]).strip()\n",
    "\n",
    "\n",
    "preprocessor = SmilesPreprocessor(atom_features=atom_featurizer, bond_features=bond_featurizer,\n",
    "                                  explicit_hs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75a2300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pre-allocating\n",
      "{'unk': 1}\n",
      "\n",
      "after pre-allocating\n",
      "{'unk': 1, \"('C', False, 0, 1, 3)\": 2, \"('C', False, 0, 3, 0)\": 3, \"('O', False, 0, 1, 0)\": 4, \"('N', False, 0, 2, 1)\": 5, \"('C', False, 0, 2, 2)\": 6, \"('C', False, 3, 4, 0)\": 7, \"('C', False, 3, 2, 2)\": 8, \"('O', False, 0, 1, 1)\": 9, \"('C', False, 4, 4, 0)\": 10, \"('C', False, 4, 2, 2)\": 11, \"('C', False, 4, 3, 1)\": 12, \"('C', False, 'max', 2, 2)\": 13, \"('C', False, 'max', 3, 0)\": 14, \"('O', False, 4, 2, 0)\": 15, \"('N', False, 3, 2, 1)\": 16, \"('C', False, 0, 2, 0)\": 17, \"('C', False, 0, 1, 1)\": 18, \"('C', True, 5, 2, 1)\": 19, \"('N', True, 5, 2, 0)\": 20, \"('C', True, 5, 3, 0)\": 21, \"('C', True, 'max', 2, 1)\": 22, \"('N', True, 'max', 2, 0)\": 23, \"('O', True, 'max', 2, 0)\": 24, \"('C', False, 3, 3, 1)\": 25, \"('C', False, 5, 2, 2)\": 26, \"('O', False, 3, 2, 0)\": 27, \"('C', False, 5, 3, 1)\": 28, \"('O', False, 5, 2, 0)\": 29, \"('C', False, 5, 3, 0)\": 30, \"('N', False, 0, 3, 0)\": 31, \"('N', False, 0, 1, 2)\": 32, \"('C', False, 0, 2, 1)\": 33, \"('N', True, 5, 3, 0)\": 34, \"('O', False, 0, 2, 0)\": 35, \"('N', False, 0, 1, 1)\": 36, \"('N', False, 0, 1, 0)\": 37, \"('N', False, 3, 3, 0)\": 38, \"('C', False, 'max', 2, 1)\": 39, \"('N', False, 4, 3, 0)\": 40, \"('C', False, 4, 3, 0)\": 41, \"('C', False, 5, 2, 1)\": 42, \"('C', False, 0, 3, 1)\": 43, \"('O', True, 5, 2, 0)\": 44, \"('C', False, 0, 4, 0)\": 45, \"('N', False, 'max', 3, 0)\": 46, \"('N', False, 5, 2, 1)\": 47, \"('C', False, 5, 4, 0)\": 48, \"('N', False, 4, 2, 1)\": 49, \"('C', False, 'max', 3, 1)\": 50, \"('O', False, 'max', 2, 0)\": 51, \"('N', False, 'max', 2, 0)\": 52, \"('N', True, 5, 2, 1)\": 53, \"('N', False, 0, 2, 0)\": 54, \"('N', False, 0, 1, 3)\": 55, \"('C', True, 'max', 3, 0)\": 56, \"('F', False, 0, 1, 0)\": 57, \"('N', False, 5, 3, 0)\": 58, \"('N', False, 5, 2, 0)\": 59, \"('N', True, 'max', 2, 1)\": 60, \"('C', False, 'max', 4, 0)\": 61, \"('N', False, 'max', 2, 1)\": 62, \"('C', True, 4, 3, 0)\": 63, \"('N', True, 4, 3, 0)\": 64, \"('N', True, 'max', 3, 0)\": 65, \"('N', False, 4, 2, 2)\": 66, \"('N', False, 0, 2, 2)\": 67, \"('N', False, 3, 2, 2)\": 68, \"('N', False, 0, 3, 1)\": 69, \"('C', False, 'max', 2, 0)\": 70, \"('C', False, 3, 3, 0)\": 71, \"('N', False, 5, 2, 2)\": 72, \"('N', False, 'max', 2, 2)\": 73, \"('N', False, 4, 3, 1)\": 74, \"('N', False, 0, 0, 3)\": 75, \"('N', False, 5, 3, 1)\": 76, \"('N', False, 4, 2, 0)\": 77, \"('N', False, 3, 3, 1)\": 78, \"('C', False, 0, 0, 4)\": 79}\n"
     ]
    }
   ],
   "source": [
    "# Initially, the preprocessor has no data on atom types, so we have to loop over the \n",
    "# training set once to pre-allocate these mappings\n",
    "print(\"before pre-allocating\")\n",
    "print(preprocessor.atom_tokenizer._data)\n",
    "\n",
    "for smiles in train:\n",
    "    preprocessor(smiles, train=True)\n",
    "    \n",
    "print()\n",
    "print(\"after pre-allocating\")\n",
    "print(preprocessor.atom_tokenizer._data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5993b",
   "metadata": {},
   "source": [
    "## Construct Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a81838fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tf.data pipeline. There's a lot of specifying data types and\n",
    "# expected shapes for tensorflow to pre-allocate the necessary arrays. But \n",
    "# essentially, this is responsible for calling the input constructor, batching \n",
    "# together multiple molecules, and padding the resulting molecules so that all\n",
    "# molecules in the same batch have the same number of atoms (we pad with zeros,\n",
    "# hence why the atom and bond types above start with 1 as the unknown class)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: ((preprocessor(row.smiles, train=False), row.h298)\n",
    "             for i, row in qm9[qm9.smiles.isin(train)].iterrows()),\n",
    "    output_signature=(preprocessor.output_signature, tf.TensorSpec((), dtype=tf.float32)))\\\n",
    "    .cache().shuffle(buffer_size=200)\\\n",
    "    .padded_batch(batch_size=64)\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: ((preprocessor(row.smiles, train=False), row.h298)\n",
    "             for i, row in qm9[qm9.smiles.isin(valid)].iterrows()),\n",
    "    output_signature=(preprocessor.output_signature, tf.TensorSpec((), dtype=tf.float32)))\\\n",
    "    .cache()\\\n",
    "    .padded_batch(batch_size=64)\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c7c7642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 23:01:19.004592: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2, 43,  2,  6, 17, 18],\n",
       "       [19, 21, 20, 19, 53,  9],\n",
       "       [ 2, 43,  9, 17, 18,  0],\n",
       "       [ 2, 17, 17, 17, 18,  0],\n",
       "       [ 2, 21, 19, 44, 19, 20],\n",
       "       [11, 11, 11, 11,  0,  0],\n",
       "       [ 2,  6,  5, 33,  4,  0],\n",
       "       [75,  0,  0,  0,  0,  0],\n",
       "       [ 2,  6,  6, 33,  4,  0],\n",
       "       [33,  4,  3, 36, 32,  0],\n",
       "       [18, 18,  0,  0,  0,  0],\n",
       "       [11, 12, 11, 15,  9,  0],\n",
       "       [ 2, 43,  9,  6,  6,  9],\n",
       "       [ 2, 21, 19, 19, 19, 44],\n",
       "       [11, 11, 12, 11,  9,  0],\n",
       "       [55,  6,  6,  3,  4,  4],\n",
       "       [ 2, 17, 18,  0,  0,  0],\n",
       "       [ 2,  3,  4, 17, 18,  0],\n",
       "       [ 2, 43,  9,  3,  2,  4],\n",
       "       [ 2, 25,  8,  7,  2,  9],\n",
       "       [18, 17, 25,  8, 27,  0],\n",
       "       [ 2, 17, 17, 43,  2,  2],\n",
       "       [ 2, 35,  6,  6,  9,  0],\n",
       "       [ 2,  6, 33,  4,  0,  0],\n",
       "       [ 2, 45,  2,  2, 17, 18],\n",
       "       [ 2,  6, 35, 43,  2,  2],\n",
       "       [18, 17,  6, 17, 18,  0],\n",
       "       [19, 53, 19, 20, 20,  0],\n",
       "       [ 2, 43, 32, 17, 37,  0],\n",
       "       [19, 20, 19, 44, 20,  0],\n",
       "       [32,  3, 32,  3,  4,  4],\n",
       "       [18, 37,  0,  0,  0,  0],\n",
       "       [19, 21, 20, 19, 44,  9],\n",
       "       [ 2, 10, 11, 12, 11, 11],\n",
       "       [18, 17,  6,  3,  4, 32],\n",
       "       [ 2, 25, 27, 25,  2,  0],\n",
       "       [18, 17,  6, 17, 37,  0],\n",
       "       [ 6,  6,  9,  9,  0,  0],\n",
       "       [ 2, 35,  3,  4, 32,  0],\n",
       "       [ 2, 12, 11, 15, 11,  0],\n",
       "       [ 2, 21, 19, 53, 19, 20],\n",
       "       [19, 19, 53, 19, 21, 32],\n",
       "       [ 2,  6,  6,  3,  4,  2],\n",
       "       [19, 21, 20, 19, 44, 32],\n",
       "       [ 2,  3,  4, 17, 37,  0],\n",
       "       [33, 36,  5, 33,  4,  0],\n",
       "       [ 6, 33,  4,  9,  0,  0],\n",
       "       [ 2,  3,  4, 33,  4,  0],\n",
       "       [ 2, 40, 11, 12, 11,  9],\n",
       "       [ 2,  6,  6,  3,  4, 32],\n",
       "       [19, 19, 20, 20, 53,  0],\n",
       "       [ 2, 54,  3, 33,  4, 32],\n",
       "       [20, 20, 20, 44, 20,  0],\n",
       "       [33,  4,  3,  4, 17, 37],\n",
       "       [ 2, 43,  2, 43,  2,  2],\n",
       "       [11, 12, 11, 41,  4,  9],\n",
       "       [ 2, 43,  2,  6, 35,  2],\n",
       "       [22, 22, 23, 22, 22, 23],\n",
       "       [ 2,  6,  3,  4,  6,  9],\n",
       "       [ 2,  5,  6, 17, 37,  0],\n",
       "       [ 2,  3,  4,  2,  0,  0],\n",
       "       [ 2, 12, 11, 11, 11,  0],\n",
       "       [79,  0,  0,  0,  0,  0],\n",
       "       [ 2,  5, 33,  4,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, outputs = next(train_dataset.as_numpy_iterator())\n",
    "inputs['atom']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b5775c",
   "metadata": {},
   "source": [
    "## Creating and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6da263f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the keras model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Input layers\n",
    "atom = layers.Input(shape=[None], dtype=tf.int64, name='atom')\n",
    "bond = layers.Input(shape=[None], dtype=tf.int64, name='bond')\n",
    "connectivity = layers.Input(shape=[None, 2], dtype=tf.int64, name='connectivity')\n",
    "\n",
    "num_features = 8  # Controls the size of the model\n",
    "\n",
    "# Convert from a single integer defining the atom state to a vector\n",
    "# of weights associated with that class\n",
    "atom_state = layers.Embedding(preprocessor.atom_classes, num_features,\n",
    "                              name='atom_embedding', mask_zero=True)(atom)\n",
    "\n",
    "# Ditto with the bond state\n",
    "bond_state = layers.Embedding(preprocessor.bond_classes, num_features,\n",
    "                              name='bond_embedding', mask_zero=True)(bond)\n",
    "\n",
    "# Here we use our first nfp layer. This is an attention layer that looks at\n",
    "# the atom and bond states and reduces them to a single, graph-level vector. \n",
    "# mum_heads * units has to be the same dimension as the atom / bond dimension\n",
    "global_state = nfp.GlobalUpdate(units=8, num_heads=1)([atom_state, bond_state, connectivity])\n",
    "\n",
    "for _ in range(3):  # Do the message passing\n",
    "    new_bond_state = nfp.EdgeUpdate()([atom_state, bond_state, connectivity, global_state])\n",
    "    bond_state = layers.Add()([bond_state, new_bond_state])\n",
    "    \n",
    "    new_atom_state = nfp.NodeUpdate()([atom_state, bond_state, connectivity, global_state])\n",
    "    atom_state = layers.Add()([atom_state, new_atom_state])\n",
    "    \n",
    "    new_global_state = nfp.GlobalUpdate(units=8, num_heads=1)(\n",
    "        [atom_state, bond_state, connectivity, global_state]) \n",
    "    global_state = layers.Add()([global_state, new_global_state])\n",
    "\n",
    "    \n",
    "# Since the final prediction is a single, molecule-level property, we \n",
    "# reduce the last global state to a single prediction.\n",
    "bde_prediction = layers.Dense(1)(global_state)\n",
    "\n",
    "# Construct the tf.keras model\n",
    "model = tf.keras.Model([atom, bond, connectivity], [bde_prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "326e8515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1466/1466 [==============================] - 372s 247ms/step - loss: 24.0283 - val_loss: 43.7520\n",
      "Epoch 2/25\n",
      "1466/1466 [==============================] - 27s 18ms/step - loss: 7.5093 - val_loss: 12.7232\n",
      "Epoch 3/25\n",
      "1466/1466 [==============================] - 25s 17ms/step - loss: 5.6623 - val_loss: 15.9044\n",
      "Epoch 4/25\n",
      "1466/1466 [==============================] - 24s 17ms/step - loss: 4.6267 - val_loss: 12.1497\n",
      "Epoch 5/25\n",
      "1466/1466 [==============================] - 25s 17ms/step - loss: 4.3876 - val_loss: 13.6361\n",
      "Epoch 6/25\n",
      "1466/1466 [==============================] - 24s 16ms/step - loss: 3.9383 - val_loss: 11.9302\n",
      "Epoch 7/25\n",
      "1466/1466 [==============================] - 25s 17ms/step - loss: 4.2883 - val_loss: 12.2021\n",
      "Epoch 8/25\n",
      "1466/1466 [==============================] - 25s 17ms/step - loss: 3.7181 - val_loss: 12.3966\n",
      "Epoch 9/25\n",
      "1466/1466 [==============================] - 24s 16ms/step - loss: 3.6344 - val_loss: 13.5050\n",
      "Epoch 10/25\n",
      "1466/1466 [==============================] - 24s 16ms/step - loss: 3.6758 - val_loss: 11.5298\n",
      "Epoch 11/25\n",
      "1466/1466 [==============================] - 24s 17ms/step - loss: 3.3417 - val_loss: 11.9088\n",
      "Epoch 12/25\n",
      "1466/1466 [==============================] - 26s 17ms/step - loss: 3.4453 - val_loss: 13.1606\n",
      "Epoch 13/25\n",
      "1466/1466 [==============================] - 25s 17ms/step - loss: 3.4568 - val_loss: 12.0766\n",
      "Epoch 14/25\n",
      "1466/1466 [==============================] - 25s 17ms/step - loss: 3.0820 - val_loss: 12.5991\n",
      "Epoch 15/25\n",
      "1466/1466 [==============================] - 26s 18ms/step - loss: 3.1826 - val_loss: 11.4401\n",
      "Epoch 16/25\n",
      "1466/1466 [==============================] - 26s 17ms/step - loss: 3.2385 - val_loss: 11.6754\n",
      "Epoch 17/25\n",
      "1466/1466 [==============================] - 25s 17ms/step - loss: 3.1664 - val_loss: 15.0697\n",
      "Epoch 18/25\n",
      "1466/1466 [==============================] - 25s 17ms/step - loss: 3.0180 - val_loss: 11.2584\n",
      "Epoch 19/25\n",
      "1466/1466 [==============================] - 28s 19ms/step - loss: 3.0897 - val_loss: 11.5286\n",
      "Epoch 20/25\n",
      "1466/1466 [==============================] - 26s 18ms/step - loss: 3.0988 - val_loss: 12.5370\n",
      "Epoch 21/25\n",
      "1466/1466 [==============================] - 27s 18ms/step - loss: 2.9605 - val_loss: 12.3400\n",
      "Epoch 22/25\n",
      "1466/1466 [==============================] - 27s 19ms/step - loss: 2.9584 - val_loss: 14.1153\n",
      "Epoch 23/25\n",
      "1466/1466 [==============================] - 26s 18ms/step - loss: 2.9877 - val_loss: 12.1773\n",
      "Epoch 24/25\n",
      "1466/1466 [==============================] - 28s 19ms/step - loss: 3.1625 - val_loss: 12.4148\n",
      "Epoch 25/25\n",
      "1466/1466 [==============================] - 27s 18ms/step - loss: 2.8758 - val_loss: 13.2586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdd253f8490>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(1E-3))\n",
    "\n",
    "# Fit the model. The first epoch is slower, since it needs to cache\n",
    "# the preprocessed molecule inputs\n",
    "model.fit(train_dataset, validation_data=valid_dataset, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eca376",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e194568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we create a test dataset that doesn't assume we know the values for the bde\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: (preprocessor(smiles, train=False)\n",
    "             for smiles in test),\n",
    "    output_signature=preprocessor.output_signature)\\\n",
    "    .padded_batch(batch_size=64)\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cdb2cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.59548044252803"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are the predictions on the test set\n",
    "test_predictions = model.predict(test_dataset)\n",
    "test_bde_values = qm9.loc[qm9['smiles'].isin(test)].h298\n",
    "\n",
    "# Calculating the MAE\n",
    "np.abs(test_bde_values - test_predictions.flatten()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c56255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
